{
  "graph": {
    "layout_direction": 0,
    "acyclic": true,
    "pipe_collision": false,
    "pipe_slicing": true,
    "pipe_style": 1,
    "accept_connection_types": {},
    "reject_connection_types": {}
  },
  "nodes": {
    "0x228d9390cb0": {
      "type_": "com.ollamaflow.nodes.StaticTextNode",
      "icon": null,
      "name": "Static Text",
      "color": [
        59,
        156,
        217,
        255
      ],
      "border_color": [
        74,
        84,
        85,
        255
      ],
      "text_color": [
        255,
        255,
        255,
        180
      ],
      "disabled": false,
      "selected": false,
      "visible": true,
      "width": 238.0,
      "height": 162.4,
      "pos": [
        251.13238746131566,
        312.0413620321571
      ],
      "layout_direction": 0,
      "port_deletion_allowed": false,
      "subgraph_session": {},
      "custom": {
        "recalculation_mode": "Dirty if inputs change",
        "text": "Say 'banana', we just need to short piece of text to test response caching.",
        "status_info": "Complete (cached)"
      }
    },
    "0x228d7f99730": {
      "type_": "com.ollamaflow.nodes.PromptNode",
      "icon": null,
      "name": "LLM Prompt",
      "color": [
        217,
        86,
        59,
        255
      ],
      "border_color": [
        74,
        84,
        85,
        255
      ],
      "text_color": [
        255,
        255,
        255,
        180
      ],
      "disabled": false,
      "selected": false,
      "visible": true,
      "width": 361.0,
      "height": 880.6,
      "pos": [
        618.4603571808518,
        108.69909308027087
      ],
      "layout_direction": 0,
      "port_deletion_allowed": false,
      "subgraph_session": {},
      "custom": {
        "recalculation_mode": "Never dirty",
        "model": "deepseek-r1:14b",
        "system_prompt": "You are a helpful assistant.",
        "user_prompt": "",
        "temperature": "0.7",
        "top_p": "0.9",
        "top_k": "40",
        "repeat_penalty": "1.1",
        "max_tokens": "2048",
        "filter_mode": "None",
        "filter_pattern": "<think>.*?</think>",
        "use_regex_flags": "true",
        "dotall_flag": "true",
        "multiline_flag": "true",
        "ignorecase_flag": "false",
        "response_preview": "<think>\nOkay, so I need to figure out how to make the system say \"banana.\" The user mentioned it's for testing response caching with a short piece of text. Hmm, maybe they want to check if repeated requests return the same cached response.\n\nFirst, I should probably think about why someone would test response caching. It's likely to ensure that once a response is generated, subsequent identical requests don't hit the database or compute again, saving resources.\n\nIn this case, the request is simple: \"Say 'banana'.\" That's straightforward, but since it's for testing, consistency is key. I need to make sure that every time someone asks the system to say \"banana,\" the response is exactly the same each time without variation.\n\nWait, should I consider any variations or different ways users might phrase this? Probably not for a caching test; it's about exact matches. So, the response should be consistent and quick because of caching.\n\nI wonder if there are any edge cases here. Maybe checking that even after some time, the cached response still holds. Or perhaps testing under load to see how the system handles multiple simultaneous requests using the cache.\n\nAlso, I need to ensure that the system doesn't mistakenly treat \"banana\" as part of a larger conversation or context. It should be isolated for this specific test.\n\nIn terms of implementation, if I were setting up the caching mechanism, I'd make sure that the key for this response is unique and easily identifiable. Maybe using the exact query string as the cache key.\n\nI should also consider how often the cache updates. If \"banana\" is a static response, there's no need to update it unless the system requires dynamic content. So, in this case, the cache can be quite straightforward with a long TTL or even indefinitely since bananas don't change.\n\nLastly, I might want to monitor the cache performance for this specific response to ensure it's being hit correctly and not evicted or causing any issues like stale data.\n</think>\n\nSure! Here's a consistent response to test caching:\n\n**Response:** banana",
        "raw_response_preview": "<think>\nOkay, so I need to figure out how to make the system say \"banana.\" The user mentioned it's for testing response caching with a short piece of text. Hmm, maybe they want to check if repeated requests return the same cached response.\n\nFirst, I should probably think about why someone would test response caching. It's likely to ensure that once a response is generated, subsequent identical requests don't hit the database or compute again, saving resources.\n\nIn this case, the request is simple: \"Say 'banana'.\" That's straightforward, but since it's for testing, consistency is key. I need to make sure that every time someone asks the system to say \"banana,\" the response is exactly the same each time without variation.\n\nWait, should I consider any variations or different ways users might phrase this? Probably not for a caching test; it's about exact matches. So, the response should be consistent and quick because of caching.\n\nI wonder if there are any edge cases here. Maybe checking that even after some time, the cached response still holds. Or perhaps testing under load to see how the system handles multiple simultaneous requests using the cache.\n\nAlso, I need to ensure that the system doesn't mistakenly treat \"banana\" as part of a larger conversation or context. It should be isolated for this specific test.\n\nIn terms of implementation, if I were setting up the caching mechanism, I'd make sure that the key for this response is unique and easily identifiable. Maybe using the exact query string as the cache key.\n\nI should also consider how often the cache updates. If \"banana\" is a static response, there's no need to update it unless the system requires dynamic content. So, in this case, the cache can be quite straightforward with a long TTL or even indefinitely since bananas don't change.\n\nLastly, I might want to monitor the cache performance for this specific response to ensure it's being hit correctly and not evicted or causing any issues like stale data.\n</think>\n\nSure! Here's a consistent response to test caching:\n\n**Response:** banana",
        "status_info": "Complete: 417 tokens",
        "full_raw_response": "<think>\nOkay, so I need to figure out how to make the system say \"banana.\" The user mentioned it's for testing response caching with a short piece of text. Hmm, maybe they want to check if repeated requests return the same cached response.\n\nFirst, I should probably think about why someone would test response caching. It's likely to ensure that once a response is generated, subsequent identical requests don't hit the database or compute again, saving resources.\n\nIn this case, the request is simple: \"Say 'banana'.\" That's straightforward, but since it's for testing, consistency is key. I need to make sure that every time someone asks the system to say \"banana,\" the response is exactly the same each time without variation.\n\nWait, should I consider any variations or different ways users might phrase this? Probably not for a caching test; it's about exact matches. So, the response should be consistent and quick because of caching.\n\nI wonder if there are any edge cases here. Maybe checking that even after some time, the cached response still holds. Or perhaps testing under load to see how the system handles multiple simultaneous requests using the cache.\n\nAlso, I need to ensure that the system doesn't mistakenly treat \"banana\" as part of a larger conversation or context. It should be isolated for this specific test.\n\nIn terms of implementation, if I were setting up the caching mechanism, I'd make sure that the key for this response is unique and easily identifiable. Maybe using the exact query string as the cache key.\n\nI should also consider how often the cache updates. If \"banana\" is a static response, there's no need to update it unless the system requires dynamic content. So, in this case, the cache can be quite straightforward with a long TTL or even indefinitely since bananas don't change.\n\nLastly, I might want to monitor the cache performance for this specific response to ensure it's being hit correctly and not evicted or causing any issues like stale data.\n</think>\n\nSure! Here's a consistent response to test caching:\n\n**Response:** banana",
        "full_filtered_response": "<think>\nOkay, so I need to figure out how to make the system say \"banana.\" The user mentioned it's for testing response caching with a short piece of text. Hmm, maybe they want to check if repeated requests return the same cached response.\n\nFirst, I should probably think about why someone would test response caching. It's likely to ensure that once a response is generated, subsequent identical requests don't hit the database or compute again, saving resources.\n\nIn this case, the request is simple: \"Say 'banana'.\" That's straightforward, but since it's for testing, consistency is key. I need to make sure that every time someone asks the system to say \"banana,\" the response is exactly the same each time without variation.\n\nWait, should I consider any variations or different ways users might phrase this? Probably not for a caching test; it's about exact matches. So, the response should be consistent and quick because of caching.\n\nI wonder if there are any edge cases here. Maybe checking that even after some time, the cached response still holds. Or perhaps testing under load to see how the system handles multiple simultaneous requests using the cache.\n\nAlso, I need to ensure that the system doesn't mistakenly treat \"banana\" as part of a larger conversation or context. It should be isolated for this specific test.\n\nIn terms of implementation, if I were setting up the caching mechanism, I'd make sure that the key for this response is unique and easily identifiable. Maybe using the exact query string as the cache key.\n\nI should also consider how often the cache updates. If \"banana\" is a static response, there's no need to update it unless the system requires dynamic content. So, in this case, the cache can be quite straightforward with a long TTL or even indefinitely since bananas don't change.\n\nLastly, I might want to monitor the cache performance for this specific response to ensure it's being hit correctly and not evicted or causing any issues like stale data.\n</think>\n\nSure! Here's a consistent response to test caching:\n\n**Response:** banana"
      }
    }
  },
  "connections": [
    {
      "out": [
        "0x228d9390cb0",
        "Text"
      ],
      "in": [
        "0x228d7f99730",
        "User Prompt"
      ]
    }
  ]
}